# IR-Superproject-2023

The integration of large language models such as BERT, GPT, and ChatGPT into search engine applications is revolutionizing the way we search for information. This themed project aims to help you understand, engage with, and advance this technology.

Through this project, you will develop an in-depth understanding of these language models and their applications in powering search engines. You will have the opportunity to explore one of several research directions that we have identified. For instance, you may choose to investigate the effectiveness of these methods under specific conditions, such as studying possible biases and robustness issues, or you design, develop, and evaluate new solutions to address known problems that affect these methods.

While completion of the INFS7410 course at UQ, or a similar Information Retrieval and Web Search course at other universities is desirable, we will provide background information and study material in the initial weeks of the project to allow you to explore these methods in depth. Therefore, if you possess a strong understanding of key artificial intelligence concepts but lack specific information retrieval knowledge, you're still encouraged to undertake this project.


## Background material and videos

Links to videos:

- 

Readings:

- [Tonellotto, N., 2022. Lecture Notes on Neural Information Retrieval. arXiv preprint arXiv:2207.13443.](https://arxiv.org/pdf/2207.13443.pdf)
- [Zhao, W.X., Liu, J., Ren, R. and Wen, J.R., 2022. Dense text retrieval based on pretrained language models: A survey. arXiv preprint arXiv:2211.14876.](https://arxiv.org/pdf/2211.14876)



## Project Directions

1. Reproduce the paper [Penha, G., Câmara, A. and Hauff, C., 2022, April. Evaluating the robustness of retrieval pipelines with query variation generators. In Advances in Information Retrieval: 44th European Conference on IR Research, ECIR 2022, Stavanger, Norway, April 10–14, 2022, Proceedings, Part I (pp. 397-412). Cham: Springer International Publishing.](https://arxiv.org/pdf/2111.13057).

2. Reproduce the paper [Chen, X., He, B., Hui, K., Sun, L. and Sun, Y., 2023. Dealing with textual noise for robust and effective BERT re-ranking. Information Processing & Management, 60(1), p.103135.](https://www.sciencedirect.com/science/article/pii/S0306457322002369).

3. Reproduce the paper [Wu, C., Zhang, R., Guo, J., Fan, Y. and Cheng, X., 2022. Are neural ranking models robust?. ACM Transactions on Information Systems, 41(2), pp.1-36.](https://dl.acm.org/doi/pdf/10.1145/3534928).
